# Ritreval Augmented Generation 1

> Bogor, 16 Desember 2025: Hujan, Noah nangis ngamuk2 mau cookies

Pada catatan ini insyaAllah akan terdapat bagaimana cara

1. Memberi indeks pada kumpulan dokumen dan bagaimana cara membersihkan (biasa disebut dengan _preprocessing_) sehingga aplikasi yg kita bentuk dapat menemukan informasi yang relevan terhadap pertanyaan.
2. Mengambil atau menarik data eksternal melalui indeks tersebut dan menggunakan sebagai konteks untuk LLM sehingga dapat menghasilkan nilai keluaran yang berlandaskan data.

Pertama catatan ini akan menjelaskan bagaimana melibatkan proses awal (_preprocessing_) pada dokumen (misalkan PDF, word, atau data dalam bentuk dokumen lainnya) kedalam format yang dapat dipahami dan dicari oleh LLM. Teknik tersebut adalah RAG, _retrieaval-augmented generation_.

Untuk menggapainya ada 4 langkah;

1. Mengambil nilai text dari dokumen.
2. Memecahnya kedalam bentuk yang teratur.
3. Mengkonversi kalimat atau kata kedalam bentuk angka yang dapat dipahami oleh komputer.
4. Menyimpan kumpulan angka representasi dari text sehingga dapat mudah dicari dan dipanggil yang berguna sebagai data atau konteks yg relevan untuk menjawab pertanyaan.

!!! note "Image"

    ![alt text](./assets/3_4_step_rag.png)

## Embeddings (Konversi text kedalam angka)

Embedding adalah sebuah proses merubah tipe data text kedalam deret angka. Namun kita tidak dapat mengembalikan deret angka tersebut ke bentuk text awalnya, maka dari itu kita kedua data tersebut disimpan.

### Embeddings sebelum adanya LLM

Lama sebelum LLM, proses embedding ini sudah dilakukan, misalkan untuk mencari full-text dari sebah website atau klasifikasi apakah sebuah email ini spam atau tidak. Contohnya dapat dilihat disini

* What a sunny day
* Such bright skies today
* I haven't seen a sunny day in weeks.

Kalimat `I haven't seen a sunny day in weeks` memliki nilai sequance `0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1` [^1]. Deret angka tersebut adalah _sparse embedding_ atau _sparse vector_. Kita dapat menggunakan _sparse vector_ untuk beberapa hal, misalkan; mencari text pada beberapa dokumen, kita dapat menghitung dari dokumen-dokumen tersebut kata kunci yang sesuai pada dokumen yang dicari.

Limitasinya model tidak memperhatikan makna dari kata, hanya apakah mengandung kata tersebut atau tidak. Misalkan kalimat _Sunny Day_ dan _Bright Skies_ adalah makna yang sama, akan tetapi dengan model embedding ini kata tersebut dianggap tidak sama.

InsyaAllah pada catatan ini juga akan dibahas terkait {==Semantic Embeddings==}, dimana metode tersebut mampu menghadapi limitasi dengan memanfaatkan deret angka dalam memahami text alih-alih hanya menggunakan arti makna satu kata.

[^1]: Lab cara menghitung _sparse vectorize_ [Embeddings Words example](./lab/00_embeddings_before_llm.ipynb)

### Embeddings pada LLM

Kita akan memahami apa itu **Embeddings Model**. Embddings model adalah salah satu cabang atau salah astu bagian dari proses pelatihan LLM. Pada awal catatan ini, kita mengetahui bahwa saat proses pelatihan (supply data) LLM dilatih  menggunakan informasi dalam bentuk kata-kata yang sangat banyak sehingga membuat model dapat memprediksi dan men-generate kalimat selanjutan dari input yang diberikan.

Kemampuan dalam melanjutkan kata ini berasal dari pemahaman dari kata makna (semantics, _understanding of meaning_),belajar untuk memahmai bagaimana kata digunakan, bagaimana makna kata digabung dengan kata lainnya. Pehaman dari makna juga dapat dirubah kedalam angka, proses embeddings yang mana dapat kita gunakan untuk beberapa tujuan tertentu. Pada praktiknya, Embeddings model ini dibangun khusus untuk tujuan tersebut sehingga kualitas dari proses embeddings ini berkualitas.

Sehingga dapat kita katakan bahwa Embeddings model adalah sebuah algoritma yang menerima nilai masukan dalam betuk text dan nilai keluaran dalam bentuk kumpulan angka. Bisa dalam bentuk _vector_ atau _dense/matrix_.


### Penjelasan Semantic Embeddings

Katakan ada 3 kata Pet, Dog dan Lion, kira2 dari ketiga kata tersebut yang mana yang mirip dengan yang lainnya. Umumnya kita akan menjawab yang mirip adalah **pet** dan **dog**. Namun komputer tidak memahami hal tersebut, ingat komputer hanya memahami angka. Dibelakang layar kata tersebut haru kita embedding untuk menghasilkan sebuah vektor. Vektor tersebut digunakan sebagai nilai input `_cosine similiarity_` untuk mengetahui kemiripan dari setiap kata. Jika nilai Cosine Similiarty 1 atau mendekati satu maka kemiripan dari kata tersebut tinggi dan jika mendekati ke atau hingga ke nilai 0 maka tingkat kemiripan rendah.

!!! note "Lab cosine similiarity"

    Lab [consine similiarty](./lab/00_cosine_similiarity.ipynb)

Lalu apa benang merahnya ?:wood::wood::wood::wood:

> Dengan kemampuan untuk merubah data dalam bentuk kata atau kalimat kedalam embeddings yang juga didalamnya ada makna dari setiap kalimat lalu mengukur kemiripan dari makna setiap kata membuat LLM dapat menemukan dokumen atau data yang relevan untuk menjawab pertanyaan.

## Merubah dokumen ke dalam text

Dokumen bisa datang dalam berbagai bentuk, PDF, text, CSV atau bisa didalam bentuk Web. Langchain memliki beberapa class loader yang dapat membantu mengambil data dari dokumen kedalam bentuk text dalam bentuk class `Document`.

!!! notes "Code"

    ```python
    from langchain_community.document_loaders import TextLoader
    text_data = TextLoader("C:/Users/Administrator/Documents/LangChainLabAssets/Data.txt")
    text_data.load()

    ## [Document(metadata={'source': 'C:/Users/Administrator/Documents/LangChainLabAssets/Data.txt'}, page_content="I'm the data inside file name Data.txt")]
    ```

    More detail lihat di lab document;

    * [txt dan CSV](./lab/6_documents_into_text.ipynb#text-loader)
    * [pdf](./lab/6_documents_into_text.ipynb#pdf-loader)
    * [web](./lab/6_documents_into_text.ipynb#web-loader)

Beberapa contoh diatas, salah satunya melakukan ekstraksi dari dokumen kedalam class `Document`. Namun ada masalah lain, yaitu bagaimana jika dokumen memiliki banyak halaman dan mengandung banyak kalimat informasi. Untuk mengatasi masalah ini kita harus memecah data tersebut kedalam potongan yang dapat diatur. Setelah pemotongan tersebutlah kita baru melakukan embedding dan melakukan pencarian makna (_semantic search_). Oke next step kita akan membahas hal tersebut.

## Splitting Text kedalam bentuk Chunks

Sekilas jika dipikirkan memang terlihat sulit untuk memecah kumpulan kalimat kedalam pecahan2 namun dengan mempertahankan makna dari setiap kata. Untungnya langchain memliki `RecursiveCharacterTextSplitter` untuk mempermudah memecah kalimat kedalam bentuk yang lebih kecil namun masih memliki makna.

Yg dilakukan oleh `RecursiveCharacterTextSplitter` adalah sebagai berikut;

1. Membuat list seperator paragraf `\n\n`, new line `\n` dan space
2. Memotong berdasarkan `chunk size`, katakan 1000, maka maksimal kalimat yang diambil tidak bisa lebih dari 1000. Jika paragraf melebih besaran `chunk size` maka yang diambil kedalam class document adalah text yang dipotong berdasarkan makna, sehingga pada class document bisa jadi berisi kurang dari 1000 karakter.
3. Jika class document sudah semuanya terpenuhi sesuai besaran `chunk size` maka class document selanjutnya berisi panjang kata (makna) sebesar `chunk_overlap`. Pelakukan yang sama yang diambil adalah kata dengan makna maksimal sebesar `chunk_overlap` tidak langsung potong.

!!! notes "code"

    [Lab Split big docs into chunks](./lab/6_documents_into_text.ipynb#Split-big-docs-into-chunks)

### Create Documents class from raw text

Katakan data yang kita miliki tidak bisa digunakan pada document loader, misalkan kita sudah memliki text mentah _raw string_. Kita dapat menggunakan method `create_documents` untuk membtuh documents class.

!!! notes "code"

    [Lab Raw Text to Document Class](./lab/6_documents_into_text.ipynb#raw-text-to-document-class)

## Generating Text Embeddings

Langchain memliki Embedding class untuk berinteraksi dengan Embeddings Model. Class tersebut menyediakan 2 fungsi;

1. Untuk embeddings document
2. Untuk embeddings query (pertanyaan)

Kita akan menggunakan interface `OllamaEmbeddings` untuk beriteraksi dengan model ollama khusus untuk embeddings[^2]. Pada catatan kali ini saya menggunakan model `nomic-embed-text`. Setelah membuat object Ollama Embeddings kita dapat memanggil fungsi `embed_documents` atau `embed_query`.

[^2]: [Text Embedding Ollama](https://docs.langchain.com/oss/python/integrations/text_embedding/ollama)

!!! notes "Lab"

    Untuk hands-on dapat melihat pada [lab embeddings text](./lab/6_documents_into_text.ipynb#generating-text-embedding-using-nomic-embed-text)

