{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43483d00-f5ed-4eda-939f-8a1b14bf6698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T03:17:19.791562Z",
     "iopub.status.busy": "2025-12-16T03:17:19.790909Z",
     "iopub.status.idle": "2025-12-16T03:17:38.127609Z",
     "shell.execute_reply": "2025-12-16T03:17:38.125909Z",
     "shell.execute_reply.started": "2025-12-16T03:17:19.791518Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3ab023-e48f-47b2-bcb3-145747f8de16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T03:17:38.132379Z",
     "iopub.status.busy": "2025-12-16T03:17:38.130731Z",
     "iopub.status.idle": "2025-12-16T03:18:08.091997Z",
     "shell.execute_reply": "2025-12-16T03:18:08.090268Z",
     "shell.execute_reply.started": "2025-12-16T03:17:38.132338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127d1cb0af5d4c6aa4d8d841a89cce69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bits_and_bytes = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bits_and_bytes,\n",
    "    torch_dtype=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8002b18d-0f8a-41cc-9999-127a52b1daa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T03:18:08.094433Z",
     "iopub.status.busy": "2025-12-16T03:18:08.093642Z",
     "iopub.status.idle": "2025-12-16T03:18:08.108873Z",
     "shell.execute_reply": "2025-12-16T03:18:08.106891Z",
     "shell.execute_reply.started": "2025-12-16T03:18:08.094389Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.1,\n",
    "    do_sample=False,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512f2d0e-0bfc-4c11-99e3-74d308ad09be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T03:18:08.113598Z",
     "iopub.status.busy": "2025-12-16T03:18:08.112563Z",
     "iopub.status.idle": "2025-12-16T03:18:34.818674Z",
     "shell.execute_reply": "2025-12-16T03:18:34.816999Z",
     "shell.execute_reply.started": "2025-12-16T03:18:08.113550Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kamu adalah asisten pembantu yang pintar\n",
      "jawab pertanyaan dengan jelas, konsisten dan hindari perulangan\n",
      "\n",
      "Siapa yang pertama kali menginjakan kaki ke bulan ?\n",
      "Jawab :\n",
      "Bumi\n",
      "\n",
      "Siapa yang pertama kali menginjakan kaki ke bulan?\n",
      "Jawab :\n",
      "Bumi\n",
      "\n",
      "Siapa yang pertama kali menginjakan kaki ke bulan?\n",
      "Jawab :\n",
      "Bumi\n",
      "\n",
      "Siapa yang pertama kali menginjakan kaki ke bulan?\n",
      "Jawab :\n",
      "Bumi\n",
      "\n",
      "Siapa yang pertama kali menginjakan kaki ke bulan?\n",
      "Jawab :\n",
      "Bumi\n",
      "\n",
      "Siapa yang pertama kali menginjakan kaki ke bulan?\n",
      "Jawab :\n",
      "Bumi\n",
      "\n",
      "Siapa yang pertama kali menginjakan\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"\"\"\n",
    "Kamu adalah asisten pembantu yang pintar\n",
    "jawab pertanyaan dengan jelas, konsisten dan hindari perulangan\n",
    "\n",
    "Siapa yang pertama kali menginjakan kaki ke bulan ?\n",
    "Jawab :\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7085dd-20b4-46ef-b579-54c233cc6f8c",
   "metadata": {},
   "source": [
    "## Palajaran penting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cf71c2-98a5-4f42-869f-0757eb7c32fe",
   "metadata": {},
   "source": [
    "* Ollama â‰  Hugging Face loader biasa\n",
    "* Ollama = model + chat template + decoding + guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50958676-1c47-465d-8267-e5acd90eed05",
   "metadata": {},
   "source": [
    "## Sekarang saya mau mencoba model 4b instrcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bacfd36-1653-4340-ab04-984e2bff7ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T03:18:34.821189Z",
     "iopub.status.busy": "2025-12-16T03:18:34.820586Z",
     "iopub.status.idle": "2025-12-16T03:18:40.753001Z",
     "shell.execute_reply": "2025-12-16T03:18:40.751352Z",
     "shell.execute_reply.started": "2025-12-16T03:18:34.821141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e865362c213b4e3993f6394bda24fad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa2131b-4a16-40f8-bdab-04a561b1bcff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T03:35:35.637570Z",
     "iopub.status.busy": "2025-12-16T03:35:35.636158Z",
     "iopub.status.idle": "2025-12-16T03:35:36.023821Z",
     "shell.execute_reply": "2025-12-16T03:35:36.021454Z",
     "shell.execute_reply.started": "2025-12-16T03:35:35.637521Z"
    }
   },
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m messages = [\n\u001b[32m      3\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: perintah}\n\u001b[32m      4\u001b[39m ]\n\u001b[32m      5\u001b[39m text = qwen_tokenizer.apply_chat_template(\n\u001b[32m      6\u001b[39m     messages,\n\u001b[32m      7\u001b[39m     tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      8\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model_inputs = \u001b[43mqwen_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_qwen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# conduct text completion\u001b[39;00m\n\u001b[32m     13\u001b[39m generated_ids = model_qwen.generate(\n\u001b[32m     14\u001b[39m     **model_inputs,\n\u001b[32m     15\u001b[39m     max_new_tokens=\u001b[32m16384\u001b[39m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:811\u001b[39m, in \u001b[36mBatchEncoding.to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[32m    807\u001b[39m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[32m    808\u001b[39m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    810\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = {\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m         k: \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[32m    812\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.items()\n\u001b[32m    813\u001b[39m     }\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    815\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "perintah = \"Siapa yang pertama kali mendarat dibulan ?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": perintah}\n",
    "]\n",
    "text = qwen_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = qwen_tokenizer([text], return_tensors=\"pt\").to(model_qwen.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model_qwen.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=16384\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8598d0-0d16-45b8-b8c2-a1f99a9817a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
