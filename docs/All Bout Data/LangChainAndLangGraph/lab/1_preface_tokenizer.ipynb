{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8c5e49-a292-43fd-a40e-44c27981d6aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T19:00:03.061565Z",
     "iopub.status.busy": "2025-08-28T19:00:03.061358Z",
     "iopub.status.idle": "2025-08-28T19:00:21.812420Z",
     "shell.execute_reply": "2025-08-28T19:00:21.811454Z",
     "shell.execute_reply.started": "2025-08-28T19:00:03.061550Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401eb88a-1f1b-4e65-ac27-459b0d0aedae",
   "metadata": {},
   "source": [
    "# Encode dari text kedalam token menggunakan tokenisasi LLama3.2 v Tiktoken\n",
    "\n",
    "More resource :\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Byte-pair_encoding\n",
    "* https://github.com/openai/tiktoken\n",
    "* https://github.com/google/sentencepiece/blob/master/python/README.md\n",
    "* https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "* https://github.com/huggingface/transformers/issues/31187\n",
    "* https://huggingface.co/docs/transformers/en/model_doc/llama3\n",
    "\n",
    "Hasil tokenisasi model Llama-3.2 sama persis dengan hasil tokenisasi `GPT-3.5` dan `GPT-4` ([playground](https://platform.openai.com/tokenizer?ref=blog.mlq.ai))\n",
    "\n",
    "Kita akan menyandingkan antara hasil tokenizer transfomer dan library tiktoken dari OpenAI library, berikut adalah daftar [Encoding Name](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80c2af-3041-4aa1-aa73-ada0250a604c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T18:38:16.172024Z",
     "iopub.status.busy": "2025-08-26T18:38:16.171825Z",
     "iopub.status.idle": "2025-08-26T18:38:16.174702Z",
     "shell.execute_reply": "2025-08-26T18:38:16.174064Z",
     "shell.execute_reply.started": "2025-08-26T18:38:16.172010Z"
    }
   },
   "source": [
    "## Tokenizer menggunakan transformer AutoTokenizerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6c657f-aaa9-40fa-97c0-ff97ba487f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T19:00:21.814302Z",
     "iopub.status.busy": "2025-08-28T19:00:21.813811Z",
     "iopub.status.idle": "2025-08-28T19:00:23.192465Z",
     "shell.execute_reply": "2025-08-28T19:00:23.191551Z",
     "shell.execute_reply.started": "2025-08-28T19:00:21.814270Z"
    }
   },
   "outputs": [],
   "source": [
    "tonkenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1ab04e-f35e-4ca7-b17d-0e2eead15581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T19:00:23.193704Z",
     "iopub.status.busy": "2025-08-28T19:00:23.193382Z",
     "iopub.status.idle": "2025-08-28T19:00:23.202658Z",
     "shell.execute_reply": "2025-08-28T19:00:23.201823Z",
     "shell.execute_reply.started": "2025-08-28T19:00:23.193676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 791, 502, 4947, 690, 1397, 682, 8420, 311, 9604, 264, 23911, 6574, 389, 6740, 520, 220, 17, 5975, 13, 578, 2128, 374, 3685, 311, 17782, 4216, 198, 438, 387, 10235, 311, 4358, 279, 14827, 2447, 22143, 13, 26982, 11, 279, 8661, 9476, 706, 1027, 42075, 311, 1893, 264, 502, 198, 23191, 3772, 4901, 555, 279, 842, 315, 279, 2046, 13, 578, 6763, 2128, 690, 1101, 5371, 5217, 4967, 389, 2027, 4519, 323, 198, 68244, 1220, 13, 578, 12432, 706, 7376, 6787, 311, 4034, 279, 5274, 1828, 2305, 311, 3449, 449, 8420, 323, 4358, 2883, 6650, 382, 644, 5552, 3754, 11, 279, 8871, 9476, 374, 47035, 279, 3622, 1887, 311, 7417, 4009, 4868, 13, 1115, 14234, 374, 3685, 311, 198, 40967, 685, 828, 5942, 8824, 323, 8108, 75954, 369, 9200, 8522, 13, 23212, 11, 279, 23096, 9476, 690, 387, 20256, 264, 198, 9376, 52499, 1567, 1828, 7884, 520, 220, 605, 6912, 11, 1405, 8420, 649, 16136, 304, 2523, 7640, 323, 11049, 449, 18105, 4994, 315, 198, 1816, 382, 791, 2883, 596, 15507, 311, 41329, 706, 1101, 1027, 27463, 11, 449, 6787, 311, 4305, 4907, 73916, 18186, 323, 198, 27369, 12571, 304, 682, 13077, 13, 578, 502, 4947, 374, 6319, 311, 12192, 9548, 1664, 33851, 11, 26206, 11, 323, 2883, 2450, 13]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The new policy will require all employees to attend a mandatory meeting on Friday at 2 PM. The team is expected to arrive early\n",
    "and be prepared to discuss the upcoming project deadline. Meanwhile, the marketing department has been instructed to create a new\n",
    "social media campaign by the end of the week. The sales team will also receive additional training on product features and\n",
    "benefits. The CEO has announced plans to visit the office next month to meet with employees and discuss company growth.\n",
    "\n",
    "In related news, the IT department is upgrading the server system to improve network security. This upgrade is expected to\n",
    "enhance data storage capacity and reduce downtime for critical applications. Additionally, the HR department will be hosting a\n",
    "team-building event next Saturday at 10 AM, where employees can participate in fun activities and bond with colleagues outside of\n",
    "work.\n",
    "\n",
    "The company's commitment to sustainability has also been highlighted, with plans to implement energy-efficient lighting and\n",
    "reduce waste in all facilities. The new policy is designed to promote employee well-being, productivity, and company success.\"\"\"\n",
    "result_encode_llama = tonkenizer.encode(text)\n",
    "print(result_encode_llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c3988-5a0f-45ec-afc4-8bcf97f3b559",
   "metadata": {},
   "source": [
    "## Tokenizer menggunakan Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f6b26c-74ae-42e5-a25d-000d4caa281f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T19:00:23.204849Z",
     "iopub.status.busy": "2025-08-28T19:00:23.204368Z",
     "iopub.status.idle": "2025-08-28T19:00:23.214900Z",
     "shell.execute_reply": "2025-08-28T19:00:23.213878Z",
     "shell.execute_reply.started": "2025-08-28T19:00:23.204831Z"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3082d51d-c591-4494-8bbe-b29846136e2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T19:00:23.216370Z",
     "iopub.status.busy": "2025-08-28T19:00:23.215995Z",
     "iopub.status.idle": "2025-08-28T19:00:23.480156Z",
     "shell.execute_reply": "2025-08-28T19:00:23.479488Z",
     "shell.execute_reply.started": "2025-08-28T19:00:23.216352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[791, 502, 4947, 690, 1397, 682, 8420, 311, 9604, 264, 23911, 6574, 389, 6740, 520, 220, 17, 5975, 13, 578, 2128, 374, 3685, 311, 17782, 4216, 198, 438, 387, 10235, 311, 4358, 279, 14827, 2447, 22143, 13, 26982, 11, 279, 8661, 9476, 706, 1027, 42075, 311, 1893, 264, 502, 198, 23191, 3772, 4901, 555, 279, 842, 315, 279, 2046, 13, 578, 6763, 2128, 690, 1101, 5371, 5217, 4967, 389, 2027, 4519, 323, 198, 68244, 1220, 13, 578, 12432, 706, 7376, 6787, 311, 4034, 279, 5274, 1828, 2305, 311, 3449, 449, 8420, 323, 4358, 2883, 6650, 382, 644, 5552, 3754, 11, 279, 8871, 9476, 374, 47035, 279, 3622, 1887, 311, 7417, 4009, 4868, 13, 1115, 14234, 374, 3685, 311, 198, 40967, 685, 828, 5942, 8824, 323, 8108, 75954, 369, 9200, 8522, 13, 23212, 11, 279, 23096, 9476, 690, 387, 20256, 264, 198, 9376, 52499, 1567, 1828, 7884, 520, 220, 605, 6912, 11, 1405, 8420, 649, 16136, 304, 2523, 7640, 323, 11049, 449, 18105, 4994, 315, 198, 1816, 382, 791, 2883, 596, 15507, 311, 41329, 706, 1101, 1027, 27463, 11, 449, 6787, 311, 4305, 4907, 73916, 18186, 323, 198, 27369, 12571, 304, 682, 13077, 13, 578, 502, 4947, 374, 6319, 311, 12192, 9548, 1664, 33851, 11, 26206, 11, 323, 2883, 2450, 13]\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding('cl100k_base')\n",
    "result_encode_tiktoken = encoding.encode(text)\n",
    "print(result_encode_tiktoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117dac46-8183-48a0-ae6b-f34b38633af6",
   "metadata": {},
   "source": [
    "## Apakah llama 3.2 menggunakan BPE tokeniser tiktoken ?\n",
    "\n",
    "Tidak dijelaskan secara terang (berdasarkan yg saya cari dari internet) yang mengatakan Llama3.2 menggunakan tiktoken dengan encoding name `cl100k_base` yang sama juga digunakan oleh model GPT-3.5 dan GPT-4 [^1](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n",
    "\n",
    "Informasi yang saya baca dari mengatakan \"_LLaMA and similar Meta models use SentencePiece with Byte-Pair Encoding (BPE).\"_ [^2](https://www.linkedin.com/pulse/transforming-ai-workflows-llama-32-simplified-text-chunking-karan-hwbqc/)\n",
    "\n",
    "\n",
    "------\n",
    "_Footer_\n",
    "\n",
    "1. https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "2. https://www.linkedin.com/pulse/transforming-ai-workflows-llama-32-simplified-text-chunking-karan-hwbqc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965602d8-d975-4a88-9e97-d3b7f6911374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T19:00:23.481848Z",
     "iopub.status.busy": "2025-08-28T19:00:23.481198Z",
     "iopub.status.idle": "2025-08-28T19:00:23.490391Z",
     "shell.execute_reply": "2025-08-28T19:00:23.489723Z",
     "shell.execute_reply.started": "2025-08-28T19:00:23.481829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_encode_tiktoken == result_encode_llama[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c517186-bcb2-4d4a-ac5f-cc32f4a02ad1",
   "metadata": {},
   "source": [
    "Saat saya mencari informasi terkait tokenisasi, saya menemukan Augustas Macijauskas, _some random people out there_  yang memperhatikan hal yang sama, tokenisasi yang digunakan Llam3 dan GPT-4. Percobaan yang beliau lakukan membandingkan tokenisasi langsung dari modelnya, dan memeriksa kesamaan vocabulary dari setiap mode kesimpulannya yang dia dapat adalah [^3](https://augustasmacijauskas.github.io/personal-website/notes/llama-3-vs-gpt-4-tokenizer/llama-3-vs-gpt-4-tokenizer.html);\n",
    "\n",
    "1. Tokenisasi HuggingFace GPT-4 mirip dengan `tiktoken` (Benar, OpenAI secara terbuka menyampaikan tokenizer yang digunakan)\n",
    "2. Llam3 dan GPT-4 memiliki vocabulary yang mirip, dikatakan 100256 token pertama sejenis.\n",
    "\n",
    "Kesimpulan yang bersangkutan tepat adanya, karena ketika disandingkan hasil tokenisasi tiktoken menggunakan encode `cl100k_base` dan tokenisasi dari _pretrained model_ hasilnya sama persis (setidaknya menggunakan sample teks yang saya gunakan).\n",
    "\n",
    "-----\n",
    "3. https://augustasmacijauskas.github.io/personal-website/notes/llama-3-vs-gpt-4-tokenizer/llama-3-vs-gpt-4-tokenizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ef8e1-cdf0-4e15-b865-7e4e604499c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T15:51:58.068154Z",
     "iopub.status.busy": "2025-08-28T15:51:58.067914Z",
     "iopub.status.idle": "2025-08-28T15:51:58.071149Z",
     "shell.execute_reply": "2025-08-28T15:51:58.070557Z",
     "shell.execute_reply.started": "2025-08-28T15:51:58.068124Z"
    }
   },
   "source": [
    "## Trying another language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c368525f-dce1-4a48-b197-a30e07394132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T19:00:23.491780Z",
     "iopub.status.busy": "2025-08-28T19:00:23.491346Z",
     "iopub.status.idle": "2025-08-28T19:00:23.497751Z",
     "shell.execute_reply": "2025-08-28T19:00:23.497127Z",
     "shell.execute_reply.started": "2025-08-28T19:00:23.491748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15225, 57668, 75146, 35304, 88852, 13177, 64479, 17885, 108, 31944, 12870, 228, 10646]\n",
      "[15225, 57668, 125510, 88852, 13177, 64479, 112494, 110778, 10646]\n"
     ]
    }
   ],
   "source": [
    "text = \"请你基于以下「评估标准」\"\n",
    "result_tiktoken_encoding_two = encoding.encode(text)\n",
    "result_llama_encoding_tokenizer_two = tonkenizer.encode(text)\n",
    "print(result_tiktoken_encoding_two)\n",
    "print(result_llama_encoding_tokenizer_two[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2724c84-9dc4-4567-895b-7d074613c6e2",
   "metadata": {},
   "source": [
    "Saya juga menemukan diskusi yang serupa. Pada diskusi tersebut sang author menanyakan, mengapa hasil tokenisasi antara tiktoken dengan seri Llma3 berbeda. Sang author menyandingkan hasil encoding pada kalimat berikut (_请你基于以下「评估标准」_). Pertanyaan tersebut terjawab, dan jawab tersebut menjadi penerangan bagi saya yang saat ini sedang belajar mengenai tokenisasi. Dibelakang layar, tokenisasi pada model Llama3 menggunakan tokenizer yang berbeda. Walaupun, ketika saya mencoba menyandingkan hasil tokenisasi dari text inggris sepanjan 1138 karakter didapatkan hasil tokenisasi yang sama.[^4](https://github.com/huggingface/transformers/issues/34403)\n",
    "\n",
    "Sekarang yang menjadi pertanyaan adalah, tokenizer apa yang digunakan oleh Meta dalam mengembangkan Llama 3 series khususnya ? beberapa sumber mengatakan menggunakan *SentencePiece*, namun sejauh ini saya belum berhasil menggunakannya.\n",
    "\n",
    "Wait, tepat sebelum saya ingin menutup laptop, saya kembali membaca dokumen model resmi dari Llama3, entah mengapa saya terlewat pada saat pencarian awal. Untuk tokensiasi, Llama3 menggunakan model _Byte Per-Encoding_ pada tiktoken dengan dengan sentuhan yang berbeda. Dimana Llama3 alih-alih memecah satu kata menjadi beberapa token, tokenisasi tersebut akan menghitungnya menjadi satu token jika kata tersebut ada didalam _vocabulary_ dari Llama. Mari kita coba\n",
    "\n",
    "----\n",
    "4. https://github.com/huggingface/transformers/issues/34403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86389857-2cb3-4862-a706-80563a80bbc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T19:00:23.498916Z",
     "iopub.status.busy": "2025-08-28T19:00:23.498521Z",
     "iopub.status.idle": "2025-08-28T19:00:23.514829Z",
     "shell.execute_reply": "2025-08-28T19:00:23.514045Z",
     "shell.execute_reply.started": "2025-08-28T19:00:23.498886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 267, 1609, 75, 278]\n",
      "[40, 267, 1609, 122591]\n",
      "\n",
      "Tiktoken\n",
      "token 40 : Decode I\n",
      "token 267 : Decode st\n",
      "token 1609 : Decode ik\n",
      "token 75 : Decode l\n",
      "token 278 : Decode al\n",
      "\n",
      "token 40 : Decode I\n",
      "token 267 : Decode st\n",
      "token 1609 : Decode ik\n",
      "token 122591 : Decode lal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_final = \"Istiklal\"\n",
    "tiktoken_result_final = encoding.encode(text_final)\n",
    "llama_tokenizer_result_final = tonkenizer.encode(text_final)\n",
    "print(tiktoken_result_final)\n",
    "print(llama_tokenizer_result_final[1:])\n",
    "\n",
    "print()\n",
    "print(\"Tiktoken\")\n",
    "for a in tiktoken_result_final:\n",
    "    print(f\"token {a} : Decode {encoding.decode([a])}\")\n",
    "print()\n",
    "for a in llama_tokenizer_result_final[1:]:\n",
    "    print(f\"token {a} : Decode {tonkenizer.decode([a])}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
