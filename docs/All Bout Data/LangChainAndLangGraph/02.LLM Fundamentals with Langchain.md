# Dasar LLM dengan LangChain

!!! tip "Coretan pada catatan ini"

    Coratan pada catatan ini dapat dilihat pada lab [disini](disini)

Tantangan dalam membuat aplikasi LLM adalah bagaimana membangun struktur perintah untuk diberikan kesebuah model dan memproses prediksi dari model sehingga mendapatkan hasil yang akurat.

!!! warning "Proses Sederhana Aplikasi LLM"
    ![alt text](./assets/2.%20Struktursederhana-aplikasi-llm.png)

InsyaAllah pada bagian catatan ini saya akan mencatat bagaimana Langchain membangun pengelompokan pemetaan terhadap LLM sehingga menjadi aplikasi LLM yang berguna.

Jadi apa itu Langchain ? adalah sebuah framework untuk membangun aplikasi LLM. Langchain adalah lapisan orkrestasi dimana mempermudah pengambangan aplikasi LLM dengan menawarkan standarisasi, penggunaan komponen berulang, dan penghubung tugas seperti "pemuatan data", "pengatturan perintah" dan "parsing hasil keluaran".

!!! tip "Bacaan lain Neural Network"
    Saya juga pernah membuat belajar dan hasil coretan saya tentang Langchain dapat dilihat di repositori berikut, [muhammadfarras/Learn-Langchain](https://github.com/muhammadfarras/Learn-LangChain/tree/main)

## Menggunakan LLM pada Langchain

!!! info "Informasi"
    Pada catatan disini saya menggunakan Ollama sebagai penyedia LLM. Sedangkan LLM yang saya gunakan adalah buatan Meta, Llama3.2. Kita bisa menggunakan provider lain atau model lain selama memliki tipe TextGeneration.

    Untuk menggunakan Ollama dengan Langchain lebih lanjut untuk setup dapat dilihat disini [https://python.langchain.com/docs/integrations/llms/ollama/](https://python.langchain.com/docs/integrations/llms/ollama/)

Sebagai pengingat, LLM adalah yang menyetir dibelakang kebanyakan Generative AI Application. Langchain menyediakan 2 interface sederhana agar kita dapat terhubung dengan API yang disediakan oleh provider LLM.

1. ChatModels
2. LLMs

Secara sederhana interface dari LLM menerima perintah dalam bentuk text dan mengirim nilai masukan tersebut ke penyedia model, lalu mengembalikan prediksi dari model sebagai nilai keluaran.

!!! notes "Code"

    ```python
    from langchain_ollama.llms import OllamaLLM

    model = OllamaLLM(model="llama3.2:latest")
    return_value = model.invoke("Assalamualaikum")
    print(return_value)
    # Wa alaykums salam. How can I assist you today?
    ```

### Chat Models

Alternatif lainnya, kita dapat menggunakan ChatModels, interface tersebut membuat kita dapat berinteraksi umpan balik secara terus menerus seperti kita sedang mengobrol. Untuk menggunakannya kita harus memahami apa itu roles.

!!! tip "Apa itu role"

    Tentang role dapat dibaca di [sini](./Other%20Resources/1_roles.md)

!!! notes "Code class dan sample chat model"

    Untuk class `ChatModelsSimplifier` dapat diakses di [ChatModelsSimplifier.py](./lab/ChatModelsSimplifier.py)

    ```python
    from ChatModelsSimplifier import MyLLMChatModel

    my_simple_model = MyLLMChatModel("llama3.2:latest")
    my_simple_model.chat_models("Halo nama saya muhammad farras ma'ruf")
    """
    Assalamu'alaikum warahmatullahi wabarakatuh, Muhammad Farras Ma'ruf.

    Saya senang bertemu dengan Anda. Apakah Anda membutuhkan bantuan atau informasi tentang topik tertentu? Atau mungkin Anda ingin berbicara tentang hobi atau minat Anda?

    Saya di sini untuk membantu dan mendengarkan apa yang Anda inginkan.
    """

    my_simple_model.chat_models("Siapa nama saya ?")
    """
    Saya ingat! Nama Anda adalah Muhammad Farras Ma'ruf.
    """
    ```

Menggunakan ChatModel, dan kita memberikan riwayat percapakan kepada model membuat model mengetahui percakapan sebelumnya.

## Template, Promp Context ? membuat model dapat digunakan berulang dan jawaban sesuai yang kita butuhkan

Perintah atau yg kita kenal dengan _prompt_ membantu model memahami konteks yang membuat jawaban lebih relevan.

Berikut adalah contoh perintah

!!! info "sample prompt"

    ```text
    Answer the question based on the context below. If the question cannot be
    answered using the information provided, answer with "I don't know".
    Context: The most recent advancements in NLP are being driven by Large Language 
    Models (LLMs). These models outperform their smaller counterparts and have
    become invaluable for developers who are creating applications with NLP 
    capabilities. Developers can tap into these models through Hugging Face's
    `transformers` library, or by utilizing OpenAI and Cohere's offerings through
    the `openai` and `cohere` libraries, respectively.
    Question: Which model providers offer LLMs?
    Answer :
    ```

Meskipun perintah seperti text biasa, akan tetapi bagi sebuah model bahas besar, perintah tersebut menjadi arahan bagaimana jawaban atas pertanyaan atau perintah tersebut dihasilkan. Perintah tidak harus kita _hardcode_, langchain menyediakan template interface yang dapat kita gunakan untuk membuat perintah secara dinamik.

??? notes "code"

    Dapat diakses di [Reusable Prompts](./lab/4_reusable_prompts.ipynb)

    ```python
    from langchain_ollama import ChatOllama
    from langchain_core.prompts import PromptTemplate
    template = PromptTemplate.from_template("""Jawab pertanyaan berdasarkan konteks dibawah ini, jika pertanyaan tidak bisa disediakan maka jawab dengan, Aku tidak mengetahui!!!%%!

    konteks: {context}

    pertanyaan: {question}

    jawab: """)

    invk = template.invoke({
        'context':'PT Venturium System Indonesia adalah perusahaan yang bergeral di bidang jasa implementasi messaging'
        ,'question':'Vensys bergerak dibidang apa ?'
    })

    print(invk.text)
    ## Result
    # Jawab pertanyaan berdasarkan konteks dibawah ini, jika pertanyaan tidak bisa disediakan maka jawab dengan, Aku tidak mengetahui!!!%%!

    # konteks: PT Venturium System Indonesia adalah perusahaan yang bergeral di bidang jasa implementasi messaging

    # pertanyaan: Vensys bergerak dibidang apa ?

    # jawab: 
    ```



## Menyetel format keluaran LLM
Terkadang plain text sangat berguna, akan tetapi ada masa dimana kita membutuhkan LLM menghasilan nilai keluaran yang lebih terstruktur, seperti JSON, XML, CSV atau dalam bentuk bahasa pemprograman seperti python atau java.

### JSON Format

Kita dapat menggunakan library Pydantic yang disediakan oleh langcahin. Caranya adalah kita membuat sebuah class turunan dari `BaseModel` lalu mendifinisikan parameter class tersebut. Pada parameter tersebut nantinya akan diisi oleh LLM sesuai dengan deskripsi atau perintah yang kita berikan. Untuk mensetup pydantic class tersebut kita dapat menggunakan method `with_output_structured`



!!! notes "Code class dan sample chat model"

    Untuk class `StructuredOutputClass` dapat diakses di [StructuredOutputClass.py](./lab/StructuredOutputClass.py)

    ```python
    from langchain_ollama import ChatOllama
    from StructuredOutputClass import JawabanTerstruktur
    import json

    model = ChatOllama(model='llama3.2:latest' , temperature=0.3).with_structured_output(JawabanTerstruktur)
    jawaban = model.invoke("Which animal has a longest neck")
    print(json.dumps(jawaban.__dict__, indent=5))
    """
    {
        "answer": "Giraffe",
        "justification": "The giraffe is known for its distinctive long neck, which can grow up to 6 feet (1.8 meters) in length."
    }
    """
    ```

## Menggabungkan apa yang kita pelajari huahuahua

Apa yang kita pelajari diatas seperti membuat chat models, membuat output keluaran (dalam bentuk JSON pada catatan ini) dan cara menggunakan prompt untuk memberikan kontek agar jawaban bisa lebih relevan.

Beberapa contoh diatas kita lebih banyak atau bahkan hanya menggunakan method `invoke`. Namun sebenarnya terdapat 2 metode interface metode lainnya,

1. Invoke, satu pertanyaan menjadi satu jawbaan
2. Batch, banyak pertanyaan menjadi banyak jawaban
3. Stream, menjawab pertanyaan langsung per-kata, stream.

!!! notes "Code"

    Dapat dilihat di [Runable Interface](./lab/5_runable_interfaces.ipynb)

### Imperative Composition
Imperative Composisition adalah sebua istilah dala membungkus sebuah kode atau komponen kedalam bentuk function atau class

!!! notes "Code"

    Untuk menggunakan `chain` kita harus mengimport dari `langchain core runnables`
    ```python
    from langchain_core.runnables import chain
    ```

    Dapat dilihat di [Imperative Composition](./lab/5_runable_interfaces.ipynb#imperative-composition).

### Declarative Composition

Pada contoh diatas, kita menggunakan dua kali invoke, pertama pada saat mendefenisikan template lalu yang kedua ketika mengirim perintah.

!!! note "sample"

    ```python
    @chain
    def askOllama(question):
        perintah = template.invoke(question)
        print (model_text.invoke(perintah).content)
    ```

Ada cara yang lebih simple, yaitu meggunakan LCEL, declarative language untuk menulis kode. Kita hanya gunakan `|`

!!! note "code"

    Dapat dilihat di [Imperative Composition](./lab/5_runable_interfaces.ipynb#declarative-language).

    ```python
    chat_model = template | model_text
    ```



